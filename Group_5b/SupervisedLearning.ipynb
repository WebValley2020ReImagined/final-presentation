{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "A supervised learning model is built using **labelled** data. There are two categories od supervised learning models, based on how the output is:\n",
    "- **regression**: continuous output *(e.g. a model extracting the direction of a street given its photo)*\n",
    "- **classification**: discrete output *(e.g. a model determining the type of a flower given its photo)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest neighbors algorithm (k-NN)\n",
    "The most basic method to achieve supervised learing, k-nearest neighbors, has actually nothing to do with *learning*, but is more about *comparing*. The whole train data set, complete with labels, is kept in memory in order for a k-NN model to work. When an unseen sample needs to be processed its **euclidean distance** to all other samples is calculated, and the `k` nearest samples are computed. A classification k-NN model would then return the **most common class** amongst them, while a regression one would return the mean (or some other composition) of the labels attached to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n",
      "[[0.         1.        ]\n",
      " [0.66666667 0.33333333]\n",
      " [0.33333333 0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# one-dimensional dataset with category 0, expect for range [2,5] with category 1\n",
    "trainDataset = [[0], [1], [2], [3], [4], [5], [6]]\n",
    "trainLabels  = [ 0,   0,   1,   1,   1,   1,   0 ]\n",
    "\n",
    "# here k is 3\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(trainDataset, trainLabels)\n",
    "\n",
    "unseenDataset = [[3.1], [-2.4], [7]]\n",
    "print(knn_classifier.predict(unseenDataset)) # prints the label predictions for the unseen samples\n",
    "print(knn_classifier.predict_proba(unseenDataset)) # prints the ratio of neighnbors with a specific label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "The cornerstone of supervised machine learnings are artificial neural networks (*ANN*). Inspired at the inner working of the **brain**, they were studied in their basic form until the eighties, but at that time computers were too slow for them to be able to learn anything useful. Only in the latest few years their usage rose significantly thanks to the much greater availability of computing power.\n",
    "\n",
    "### Structure\n",
    "A neural network is made of basic units called *neurons*. Each aritificial neuron is fed with **one or multiple inputs** (which are combined by taking their sum) from which it calculates an output (i.e. the *activation* of a neuron) by applying an ***activation function*** (such as `tanh`, `ReLU` or `sigmoid`). This behaviour is similar to that of a natural neuron, which takes electric impulses as input and activates only if the total impulse is above a certain threshold. A classical neural network is composed of **multiple layers of neurons**, where the first one is the *input layer*, the last one is the *output layer*, and every other optional layer in between is a *hidden layer*. Each layer is fully connected to the next one with weighted edges.\n",
    "\n",
    "### Feedforward\n",
    "When an input needs to be processed, it is assigned to the activations of the input layer. Then the *feedforward* process takes place: each neuron's activations in the current layer are **multiplied by the weights** of the edges going to the next layer's neurons, then the activations of the **next layer** are calculated and the process repeats on the next layer. At the end the **continuous** output can be found in the activations of the neurons in the output layer. A ***classification*** neural network usually has as many output neurons as there are *classes*, and the value (usually restricted in range 0 to 1) found in each neuron represents the **probability** for that class to be the corrected one, as predicted by the network.\n",
    "\n",
    "### Backpropagation\n",
    "During the training process the network is able to learn from wrongly predicted outputs thanks to the backpropagation process. The label attached to the input sample, with the same number of dimensions as there are output neurons, also called *expected output*, is used to calculate how off the *predicted output* is from the expected one. Then every "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural networks\n",
    "The bleeding edge applications of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub>Notebook by Fabio Giovanazzi (@Stypox)</sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
